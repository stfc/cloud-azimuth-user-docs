{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>STFC's Azimuth platform is powered by Azimuth, a self-service portal for managing cloud resources with a particular focus on simplifying the use of cloud for scientific computing, high-performance computing (HPC) and artificial intelligence (AI).</p> <p></p> <p>If this is your first visit to the Azimuth portal, or you would just like to know more about how to get access to a cloud project - get started with Azimuth science platforms.</p>"},{"location":"#disclaimer","title":"Disclaimer","text":"<ul> <li>The STFC Cloud Azimuth platform is still in development, therefore data loss could occur, and it is recommended that no data be stored on there that you do not wish to lose.</li> <li>Updates/patches of the platform may come at short notice, but we endeavour to inform users one week in advance whenever possible.</li> <li>Support for Azimuth-related issues is only offered during regular working hours, out-of-hours support is not currently being provided.</li> <li>For any issues or for feedback please contact us via STFC Cloud Support: cloud-support@stfc.ac.uk</li> <li>Use during the early-access preview period is limited to parties of the STFC Cloud Azimuth Platform Memorandum of Understanding</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#accessing-azimuth","title":"Accessing Azimuth","text":"<p>Azimuth can be accessed here:</p> <p>https://portal.apps.cape.stfc.ac.uk</p> <p>Access to an STFC Cloud OpenStack project is required to use Azimuth.</p> <p>You log in as you would normally to the STFC Cloud OpenStack interface, either using FedID or IAM authentication. This will then take you to a screen where you select which of your projects you want to work in.</p> <p>A cloud project (or cloud tenancy) is an organizational unit in the cloud to which users are assigned. Users can be members of one or more projects. </p> <p>Cloud resources belong to a project and not a user. This means that all resources within a project are visible, editable and accessible by all members of the project.</p> <p>To be allocated a cloud project, or if you are unable to access an existing project when logging in with FedID or IAM as you usually would for OpenStack, please contact cloud support or your project coordinator.</p> <p>Other users can be granted access to all or specific platforms deployed on Azimuth, without having access to the Azimuth management dashboard itself, through the Identity Provider.</p>"},{"location":"#platforms","title":"Platforms","text":"<p>Platforms can be deployed directly from the Azimuth Platforms interface:</p> <p></p>"},{"location":"#kubernetes","title":"Kubernetes","text":"<p>A fully-featured Kubernetes container orchestration cluster.</p>"},{"location":"#kubernetes-applications","title":"Kubernetes Applications","text":"<p>Kubernetes Applications require an existing Kubernetes cluster before they can be deployed.</p>"},{"location":"#argocd","title":"ArgoCD","text":"<p>ArgoCD provides a declarative GitOps Continuous Delivery (CD) platform for Kubernetes clusters.</p>"},{"location":"#binderhub","title":"BinderHub","text":"<p>BinderHub allows a reproducible set of Jupyter notebooks to be deployed onto a JupyterHub instance.</p>"},{"location":"#daskhub","title":"DaskHub","text":"<p>A multi-user DaskHub on Kubernetes. Dask is a flexible library for parallel computing in Python.</p>"},{"location":"#jupyterhub","title":"JupyterHub","text":"<p>A multi-user JupyterHub on Kubernetes, powered by zero-to-jupyterhub.</p>"},{"location":"#kubeflow","title":"KubeFlow","text":"<p>KubeFlow is a machine learning toolkit for Kubernetes clusters, using Jupyter Notebooks and TensorFlow.</p>"},{"location":"advanced/machines/","title":"Machines","text":"<p>Danger</p> <p>Stopping or deleting machines using this page may cause potentially unrecoverable issues with platforms that they are part of. Do not stop or delete machines here unless you know what you are doing!</p> <p>The machines tab allows a subset of the Instance management options available to you in the OpenStack Web UI (Horizon) to be performed within Azimuth. Machines in your OpenStack Project/Tenancy can be created or managed, within your project quota. This page can also be used to easily check the status of machines deployed as part of platforms.</p> <p>New machines will have your SSH Key authorized, set using the \"SSH Public Key\" link on Azimuth's sidebar. Note that changing the key here will not update authorized keys on existing machines.</p> <p></p>"},{"location":"advanced/volumes/","title":"Volumes","text":"<p>Danger</p> <p>Detatching or deleting volumes using this page may cause potentially unrecoverable issues with platforms that they are part of. Do not detach or delete volumes here unless you know what you are doing!</p> <p>The Volumes page provides a subset of the Volume options available in the OpenStack Web UI (Horizon) within Azimuth. Disk volumes can be created, deleted, attached and detached within your project/tenancy. This is useful, for example, to check the status of volumes in use by deployed platforms. </p> <p></p>"},{"location":"identity_provider/identity_provider/","title":"Identity Provider","text":""},{"location":"identity_provider/identity_provider/#introduction-to-identity-management","title":"Introduction to Identity Management","text":"<p>Azimuth has two kinds of users: \"Platform Admins\" who are able to sign into Azimuth, manage deployed platforms in a tenancy, and administer access to those platforms; and \"Platform Users\", who can access one or more platforms deployed onto Azimuth.</p> <p>Only Platform Admins require an OpenStack account. This is because each azimuth tenancy (i.e. OpenStack Project) has an associated \"realm\" in Keycloak. Keycloak is an Identity Management Platform, which allows platform access to be granted granularly per user.</p> <p>For example, say you are running a workshop on JupyterHub. Using Keycloak, a project admin can create users in the associated realm for each workshop attendee. These attendees can then only be granted access to one specific JupyterHub deployment for the workshop, after which their access can be revoked.</p> <p>Important concepts in Keycloak are Users, Clients, Realm Roles and Groups</p> Concept Definition Users Individuals who can authenticate into the platform. In Keycloak, users can be created per realm, allowing fine-grained access control specific to each Azimuth tenancy. Clients Applications or services (e.g. JupyterHub) that users can access through Keycloak. Clients are configured within a realm and represent the endpoints that Keycloak secures. Realm Roles Permissions that can be assigned to users within a realm. These define what actions a user can perform and are often tied to specific clients or administrative functions. Groups Collections of users within a realm that allow for bulk assignment of roles and permissions.Each realm typically includes at least two groups: <code>admins</code> and <code>platform-users</code>, along with unique roles for each deployment.<code>admins</code> have admin status for a realm, allowing them to perform actions on the Keycloak admin console.<code>platform-users</code> are granted access to every deployment in the realm. Alternatively, the unique groups for each deployment can be used to grant access to a single deployment. Additionally, subgroups are created which can grant access to a single service under that deployment."},{"location":"identity_provider/identity_provider/#groups-example","title":"Groups Example","text":"<p>Consider a Kubernetes cluster platform deployed under azimuth, <code>my-binderhub</code>. Two services are exposed under it if enabled, BinderHub and JupyterHub.</p> <ul> <li><code>platform-users</code> have access to both services, and every other deployment in the tenancy.</li> <li>Users in the <code>kubeapp-my-binderhub</code> have access to both services, BinderHub and JupyterHub, but no other deployments.</li> <li>Users in the <code>kubeapp-my-binderhub/my-binderhub-binderhub-azimuth-jupyterhub</code> group have access to the JupyterHub, but not BinderHub.</li> </ul> <p>Admins can add users to these individual groups through the Keycloak dashboard, hence granting them access to individual services.</p>"},{"location":"identity_provider/identity_provider/#bulk-importing-users","title":"Bulk Importing Users","text":"<p>Bulk Importing/Generating a set of users can be achieved via a partial import to your Keycloak realm.</p> <p>Here is a simple process to do so:</p> <ol> <li>Download this helper script csvToJsonConverterScript.sh</li> <li>Set the script as executable with <code>chmod +x csvToJsonConverterScript.sh</code></li> <li>Create a CSV file (in a spreadsheet editor, or a text editor), with the following format, i.e.:</li> </ol> <pre><code>username/*(1)!*/,firstName/*(2)!*/,lastName/*(3)!*/,email/*(4)!*/,password/*(5)!*/,temporary/*(6)!*/,require_idp_link/*(7)!*/,realmRoles/*(8)!*/,groups/*(9)!*/\nmpython,Monty,Python,mpython@example.com,Password123,false,false,some_role,some_group\ngzilla,God,Zilla,gzilla@example.com,Password456,true,true,some_role;some_second_role,some_group;some_second_group\n</code></pre> <ol> <li>Username. If using IAM authentication, this should be FedID.</li> <li>First Name</li> <li>Last Name</li> <li>Email Address</li> <li>Initial Password</li> <li>One of <code>true</code> or <code>false</code>. If <code>true</code>, the user will be prompted to change their password.</li> <li>One of <code>true</code> or <code>false</code>. If <code>true</code>, the user will be prompted to connect their account to an identity provider, i.e. Azimuth's Keystone-based default provider, or a custom provider you have set up i.e. IAM.</li> <li>A semicolon separated list of realm roles to assign to the user</li> <li>A semicolon separated list of groups to assign to the user</li> </ol> <p></p> <ol> <li>Run the script to generate an equivalent JSON file with <code>./csvToJsonConverterScript.sh &lt;input_file&gt; &lt;output_file&gt;</code><ol> <li>I.e. <code>./csvToJsonConverterScript.sh myCreatedUserCSV.csv someOutputJsonFile.json</code></li> </ol> </li> <li>On Keycloak, go to Realm Settings </li> <li>Click Actions, then Partial Import in the top left </li> <li>Browse for and upload the JSON output, and check it for sanity, i.e. check the number of users, and the properties of the json </li> <li>Click import, and wait for it to complete</li> <li>Check the Users tab for the created users</li> </ol>"},{"location":"identity_provider/identity_provider/#adding-an-iam-based-identity-provider","title":"Adding an IAM-Based Identity Provider","text":"<p>Warning</p> <p>This process hasn't yet been fully tested and explored against user use cases. Please reach out with comments, improvements or issues; and carefully consider each step</p> <p>In addition to allowing login via the same account interface as OpenStack (via Keystone, through the Azimuth identity provider), an additional custom identity provider can be setup.</p> <p>This can, for example, use IRIS-IAM/SKA-IAM. Users can then link their accounts to allow logging in through IAM; or groups can be assigned based on IAM groups (for example, to allow access to JupyterHub for all users in a research group)</p> <p>To set this up:</p> <ol> <li>On Keycloak, open Identity Providers from the sidebar</li> <li>Press Add Provider \u2192 OpenID Connect v1.0</li> <li>Set the Alias to something identifiable, i.e. \"iris-iam\"</li> <li>Set the Display Name to be a human-readable equivalent, i.e. \"IRIS-IAM\" </li> <li>Copy the Redirect URI</li> <li>Visit your IAM interface, for example https://iris-iam.stfc.ac.uk/ or https://ska-iam.stfc.ac.uk/, and login</li> <li>Visit \"My Clients\" in the top left, and press New Client</li> <li>Add a Redirect URI from the URI you copied from Keycloak</li> <li>Set the client name, description, and contact email</li> <li>In the Scopes tab, ensure <code>openid</code>, <code>profile</code>, <code>email</code>, and <code>preferred_username</code> are checked</li> <li>In the Credentials tab, set <code>Token endpoint authentication method</code> to <code>Client secret over HTTP POST authentication</code></li> <li>Save the client</li> <li>In the Main tab, copy the Client ID</li> <li>Back in Keycloak, paste the Client ID</li> <li>In IAM, in the credentials tab, copy the Client Secret</li> <li>In Keycloak, paste the Client Secret</li> <li>Use Discovery Endpoint is checked, then use one of: <code>https://iris-iam.stfc.ac.uk/.well-known/openid-configuration</code> <code>https://ska-iam.stfc.ac.uk/.well-known/openid-configuration</code></li> <li>Ensure Client Authentication is set to \"Client secret sent as post\"</li> <li>Press Add</li> <li>Open the newly added provider</li> <li>Under OpenID Connect settings \u2192 Advanced, set \"Scopes\" to <code>openid profile email preferred_username</code></li> <li>Under \"Advanced Settings\", set \"First login flow override\" to \"azimuth first login\"<ol> <li>This is important as this authentication flow disables the \"Review Profile\" step which is present in the default broker login flow. Otherwise, users could create new users with any username of their choice.</li> </ol> </li> <li>Press Save</li> </ol> <p>You now have a minimally functional login provider, allowing user accounts to be linked to IAM.</p> <p>Existing users from the Azimuth provider (i.e. yourself) should be prompted to link their account to IAM when they log in.</p>"},{"location":"identity_provider/identity_provider/#manually-adding-users","title":"Manually Adding Users","text":"<p>The default behaviour for new users logging in with IAM is to create them an account automatically. This user account can then be granted access to services manually via groups.</p> <p>Alternatively, users can be pre-created then connect their account to IAM upon first login. A new user with their username and a temporary password can be created and assigned groups. When logging in for the first time with IAM, the account matching their username will link to it.</p>"},{"location":"identity_provider/identity_provider/#restrict-access-to-a-certain-iam-group-and-assign-a-keycloak-group","title":"Restrict access to a certain IAM group, and assign a Keycloak group","text":"<p>As stated above, the default behaviour for an unknown user is to automatically create a Keycloak account. Access to IAM can be restricted to users with a specific <code>claim</code>, for example IAM group or profile information like email address. The following example shows how to restrict login based on the <code>groups</code> claim.</p> <p>Note</p> <p>If an IAM group doesn't yet exist for your team, you should contact your IAM team for support in creating one.</p> <ol> <li>Under Keycloak \u2192 Identity Providers, open your custom OpenID identity provider for IAM</li> <li>Under Advanced Settings, enable \"Verify essential Claim\"</li> <li>For Essential claim, enter <code>groups</code><ol> <li>For other use cases, this can be any claim allowed by your set of OpenID scopes. See the INDIGO IAM documentation.</li> </ol> </li> <li>Essential claim value can take a RegEx string. For example, <code>.*stfc-cloud.*</code> to require all users be in the <code>stfc-cloud</code> IAM group. Remember to properly escape RegEx characters, and be aware this example string allows for any group name that contains the sub-string <code>stfc-cloud</code>.</li> </ol> <p></p>"},{"location":"identity_provider/identity_provider/#assign-an-iam-group-to-a-keycloak-group","title":"Assign an IAM group to a Keycloak group","text":"<p>In addition to restricting login based on <code>claim</code>, Mappers can be used to assign Keycloak groups or other user properties based on them. For example, to assign all users in the <code>stfc-cloud</code> group access to a service like JupyterHub</p> <ol> <li>Under Keycloak \u2192 Identity Providers, open your custom OpenID identity provider for IAM</li> <li>At the top, visit the Mappers tab and press Add Mapper</li> <li>If as above you have already restricted access via Essential Claim to your group, you can simply select Hardcoded Group \u2192 Select your JupyterHub access group<ol> <li>Alternately, there are more advanced mapper types, some of which themselves support RegEx requirements</li> <li>For example, to allow all users to login but only allow a certain IAM group to access JupyterHub:<ol> <li>Mapper type: Advanced Claim to Group</li> <li>\"Add Claims\" \u2192 Key <code>groups</code>, value <code>.*stfc-cloud.*</code></li> <li>Enable \"RegEx claim values\"</li> <li>Set the JupyterHub service-specific group to grant at the bottom</li> </ol> </li> </ol> </li> <li>Press Save, after naming it</li> </ol>"},{"location":"known_issues/known_issues/","title":"Known Issues","text":"<p>The following is a list of known issues and potential workarounds for Azimuth. If something isn't listed here, please provide us feedback</p>"},{"location":"known_issues/known_issues/#quotas-prediction-may-not-be-accurate","title":"Quotas prediction may not be accurate","text":"<p>Azimuth's prediction of quota usage does not always properly calculate figures like RAM, and it doesn't consider i.e. number of security groups.</p>"},{"location":"known_issues/known_issues/#workaround","title":"Workaround","text":"<p>Care should be taken, for example using the OpenStack Web UI, that any deployed clusters fit into project quotas.</p> <p>If this was not the case, you should request a quota is increased and update/upgrade the cluster via Azimuth.</p>"},{"location":"known_issues/known_issues/#some-apps-can-fail-to-deploy-or-notebooks-fail-to-launch-with-low-worker-cpus","title":"Some apps can fail to deploy, or notebooks fail to launch, with low worker CPUs","text":"<p>Apps may fail to deploy if your cluster has a limited count of workers; and this isn't obvious other than checking logs via the Monitoring System, or if the app shows logs in its web UI (i.e. JupyterHub launching a server).</p> <p>In future, we will investigate a clearer way to alert users of this situation.</p>"},{"location":"known_issues/known_issues/#workaround_1","title":"Workaround","text":"<p>Make use of Auto Scaling on Worker node groups to ensure Azimuth can properly scale more worker nodes as needed to fill CPU reservation requirements. Be aware that, due to the default set of required Cluster Addons, and default apps (Monitoring/Kubernetes Dashboard), even idle clusters may already have too much compute served to allow apps to run with only one worker node.</p> <p>If the maximum count of worker nodes has been autoscaled, that indicates Azimuth has a lot of compute reserved on this cluster. The maximum count should be increased, requesting an increase of OpenStack quota if required.</p>"},{"location":"known_issues/known_issues/#gpu-node-labels-take-some-time-to-apply","title":"GPU Node Labels take some time to apply","text":"<p>Newly created GPU-enabled node groups may take some time to apply relevant GPU-specific node labels. This may cause issues with some apps not properly detecting GPU availability.</p>"},{"location":"known_issues/known_issues/#workaround_2","title":"Workaround","text":"<p>You should wait 5-10 minutes after creating a GPU-enabled worker node for feature discovery and driver setup to finish running.</p> <p>You can follow the progress with <code>kubectl</code>. After connecting via the clusters kubeconfig, run <code>watch \"kubectl get nodes -o=custom-columns=NAME:.metadata.name,GPUs:.status.capacity.'nvidia\\.com/gpu'\"</code> and wait for the worker node to show available capacity.</p>"},{"location":"known_issues/known_issues/#quota-usage-estimates-are-incorrect-for-kubernetes-clusters","title":"Quota Usage Estimates are Incorrect for Kubernetes Clusters","text":"<p>Azimuth's estimates for quota usage when deploying or updating clusters are frequently incorrect, and do not consider every restricted resource type like Network Security Groups. This may mean clusters are deployed which will not successfully provision, with nodes getting stuck in Provisioning or Pending without obvious errors.</p>"},{"location":"known_issues/known_issues/#workaround_3","title":"Workaround","text":"<p>A temporary hotfix has been applied which should resolve quota estimation for CPU, RAM and Machine Count while we wait for a proper upstream fix. Users should manually and carefully check the remaining quotas via the OpenStack Web UI (Horizon), especially Disk Volume usage and the different Networking usage restrictions for example security groups count and security group rules count.</p>"},{"location":"platforms/jupyter-notebook/","title":"Jupyter notebook","text":""},{"location":"platforms/jupyter-notebook/#introduction","title":"Introduction","text":"<p>The Jupyter Notebook platform opens an existing Jupyter notebook repository in an executable environment on a cloud instance, making the notebook code immediately reproducible. The notebook is accessed via a web browser.</p> <p>The Jupyter notebook that you want to launch should be available from a repo2docker-compatible repository, hosted at GitHub, GitLab, Zenodo, Figshare or Dataverse. The underlying platform functions similarly to Binder, and is able to launch any notebook repository that conforms to the Reproducible Execution Environment Specification (REES).</p> <p>The Jupyter notebook instance has a cloud volume (or virtual disk) available at <code>/data</code>, which may be useful for working on  large datasets that cannot be distributed with the notebook repository because of their size. The capacity of this cloud volume is configurable when launching the platform.</p>"},{"location":"platforms/jupyter-notebook/#launch-configuration","title":"Launch configuration","text":"<p>Warning</p> <p>Platforms and their names are visible to all members of the cloud project!</p> Option Explanation Platform name A name to identify the Linux Workstation platform. Notebook repository A REES compliant Jupyter notebook repository URL. Jupyter notebook size The size of the cloud instance to run the Jupyter notebook. The options in this menu are set by the cloud operator, and the number of CPUs and quantity of RAM are displayed for each size. Volume size The size (in Gigabytes) of the cloud volume (virtual disk) available at <code>/data</code>."},{"location":"platforms/jupyter-notebook/#advanced","title":"Advanced","text":""},{"location":"platforms/jupyter-notebook/#platform-monitoring","title":"Platform monitoring","text":"<p>A Grafana dashboard for system monitoring is included in the platform, and is accessible from the platforms page. General current and historical system information is visible.</p>"},{"location":"platforms/kubernetes/","title":"Overview","text":""},{"location":"platforms/kubernetes/#introduction","title":"Introduction","text":"<p>The Kubernetes platform provides a fully-featured Kubernetes container orchestration cluster, with included monitoring, ingress and application dashboards. It can be used for running Kubernetes applications within Azimuth, or custom installations using either Helm Charts or Kustomize to manage Kubernetes manifests.</p> <p>A kubeconfig file for use with <code>helm</code> or <code>kubectl</code> is provided in platform Details after the platform has been launched.</p>"},{"location":"platforms/kubernetes/#launch-configuration","title":"Launch configuration","text":"<p>Warning</p> <p>Platforms and their names are visible to all members of the cloud project!</p> <p>Bug</p> <p>Azimuth currently incorrectly calculates quota predictions for new clusters. If a cluster doesn't fit within your quota, nodes may fail to create and become stuck in \"Provisioning\" or \"Pending\" without warning messages.Please manually ensure you have enough OpenStack quota for any created clusters, especially Disk Volume and Network Security Groups quota, until this issue is resolved. As an additional issue, Azimuth's Quota tab doesn't include every quota limit. For the time being, check using OpenStack's Web UI (Horizon).</p> <p>To get started, in the Platforms tab, press the  New Platform button, and select Kubernetes.</p> <p>Or, when prompted to select a Kubernetes cluster when deploying any other platform, one can be quickly made using the green plus button:</p> <p></p> <p>You will then be presented with launch configuration options to fill in:</p> <p></p> Option Explanation Cluster name A name to identify the Kubernetes cluster platform. Cluster template The cluster template defines the deployed Kubernetes version. We aim to keep the most recent minor versions. When older ones are removed, the cluster version will be marked Deprecated and can be Upgraded. Control plane size The size of the cloud instances to run the Kubernetes control plane. The options in this menu are your available flavours in OpenStack, and the number of CPUs and quantity of RAM are displayed for each size."},{"location":"platforms/kubernetes/#node-groups","title":"Node groups","text":"<p>Warning</p> <p>All Kubernetes clusters must contain a least one node group of Kubernetes worker nodes. More worker nodes may be needed to deploy more apps, or for some apps to operate especially when they reserve resources. For example, JupyterHub will try to reserve a CPU core for each Jupyter server, hence servers may fail to deploy without enough worker nodes. To mitigate issues with having too few worker nodes, Autoscaling can be used as described below. If the maximum worker count is reached, this indicates you may need to increase this maximum limit to match load requirements. </p> <p>Info</p> <p>If you are creating a Kubernetes cluster to use with applications which require GPU, be sure to select a Node Size that contains GPUs.</p> Option Explanation Name A name to identify the node group. Names are available inside the Kubernetes cluster as node labels. Node size The size of the cloud instances that form the node group. The options in this menu are your available flavours in openstack, and the number of CPUs and quantity of RAM are displayed for each size. Cloud sizes may also dictate access to other hardware, such as GPUs or high-speed network interfaces. Enable autoscaling for this node group? When autoscaling is selected, the amount of cloud instances in the node group will increase when existing resources are not sufficient to run the requested amount of pod resources. As the amount of requested pod resources declines, cloud instances are removed from the node group.When autoscaling is not selected, the size of the node group remains fixed. Node Count When autoscaling is selected, the minimum and maximum amount of cloud instances to allow in this node group. When autoscaling is not selected, the fixed amount of cloud instances in this node group."},{"location":"platforms/kubernetes/#cluster-addons","title":"Cluster addons","text":"<p>Info</p> <p>By default all cluster addons are enabled, they provide useful information about the state of your Kubernetes cluster and add additional functionality.</p> Option Explanation Enable Kubernetes Dashboard? The Kubernetes dashboard will be available from the platforms page. Enable cluster monitoring? A Grafana instance with pre-configured dashboards for visualizing cluster telemetry will be available from the platforms page."},{"location":"platforms/kubernetes/#advanced-options","title":"Advanced Options","text":"<p>Danger</p> <p>Advanced options are set to reasonable defaults and changing them may result in unexpected behaviour, including Kubernetes clusters failing to deploy. Do not change these unless you know what you are doing!</p> <p>Warning</p> <p>Volumes cannot be shrunk after they are created, care should be taken in sizing them sensibly if they are changed from the default size.</p> Option Explanation Enable auto-healing? If enabled, the cluster will try to remediate unhealthy nodes automatically. Enable Kubernetes Ingress? Installs a Kubernetes Ingress to expose user-created services in the cluster via a load balancer. Requires an external IP for the load balancer to be allocated in OpenStack. This is not needed for the apps and platforms deployed by Azimuth as it exposes services automatically using its Zenith proxy, access controlled by the Identity Provider, Keycloak. Metrics volume size The size of the openstack volume allocated to store cluster metrics. 10GB is a sensible default. Metrics are retained for 90 days, or until the volume is full, whichever happens first. Logs volume size As above, but the size of the openstack volume used to store logs. 10GB is a sensible default. Logs are retained for only 72 hours. Unlike metrics, if the volume is full, logs will no longer be recorded and existing volumes may become corrupted. <p>Info</p> <p>Your cluster will show as \"Reconciling\" when it is first deployed, or changes are made. This is Kubernetes terminology, meaning it is being changed in some way to match a specification. When it is fully installed, it will show as \"Ready\". When a cluster component shows as \"Unhealthy\", it means its configuration isn't matching expectations. This could be it has not yet finished installing/changing, or it could be experiencing an error which would need investigating in logs with the Kubernetes Dashboard or via <code>kubectl</code>. Running an update/upgrade without changing anything may trigger Kubernetes to automatically reconcile and fix it.</p>"},{"location":"platforms/kubernetes/#accessing-deployments","title":"Accessing Deployments","text":""},{"location":"platforms/kubernetes/#cluster-deployment","title":"Cluster Deployment","text":"<p>If the Kubernetes Dashboard app was installed, the easiest way to manage a cluster deployment is through the Kubernetes Dashboard link on the Details page for the cluster. Access to this can be managed through the Azimuth Identity Provider.</p> <p>Alternatively, for advanced users, the Kubeconfig for the cluster can be accessed from the Details page, using the button at the top.</p> <p>This can be used alongside tools like kubectl or helm for cluster management.</p> <p>To do so, the Kubeconfig should be downloaded, then the <code>KUBECONFIG</code> environment variable set to point to it. Alternatively, for permanent usage, it can be moved and renamed to a file named <code>config</code> at <code>$HOME/.kube/config</code>. For information on how to use Kubeconfig files, see the kubernetes docs. </p>"},{"location":"platforms/kubernetes/#kubernetes-app-deployments","title":"Kubernetes App Deployments","text":"<p>Warning</p> <p>Anyone with access to the project will be able to log into any of the Azimuth created deployments in the project</p> <p>Kubernetes Applications deployed onto a cluster are exposed by Azimuth's Zenith proxy. They are made available publically under a subdomain, and access can be controlled via the Identity Provider.</p> <p>To deploy additional platforms, use the same  New Platform button</p> <p>To access them once deployed, navigate to their Platform Details page, then click the link under \"Services\".</p> <p>If enabled under cluster addons in the details of the cluster, Monitoring or the Kubernetes Dashboard can be accessed through the Services section of the Cluster's own Details page.</p>"},{"location":"platforms/kubernetes/#patching-deployments","title":"Patching Deployments","text":"<p>Info</p> <p>In cases where we require users to patch their Azimuth deployments, we will let users know through the usual STFC Cloud communications channels</p> <p>Warning</p> <p>This may cause your deployment to be rebuilt, and may result in data loss. Further testing (and feedback) is required to identify and resolve potential causes of data loss.</p> <p>The STFC Cloud team will periodically push changes to the Azimuth images and deployments. We aim to keep the most recent minor versions of Kubernetes. When older ones are removed, the cluster version will be marked Deprecated. In order to update your deployments, click the orange  <code>Upgrade</code> button on your instance details. Machines that need patching, ones that are using deprecated versions, will be outlined in red.</p> <p>Changes can be made to cluster configurations using the green  <code>Update</code> button; for example to make changes to node groups or enable additional addons.</p>"},{"location":"platforms/kubernetes/#deleting-deployments","title":"Deleting Deployments","text":"<p>Deployments can be deleted using the red  <code>Delete</code> button in Details.</p> <p>Warning</p> <p>Kubernetes apps deployed onto it will also be deleted.</p>"},{"location":"platforms/linux-workstation/","title":"Linux workstation","text":""},{"location":"platforms/linux-workstation/#introduction","title":"Introduction","text":"<p>The Linux Workstation platform provides a flexible Ubuntu 22.04 cloud instance, with web-browser access to the graphical desktop or shell. Optionally, users may add an external IP address, which allows the instance to be accessed from a machine outside of the project using SSH.</p> <p>The workstation instance has a cloud volume (or virtual disk) available at <code>/data</code>, which may be useful for working with large datasets. The capacity of this cloud volume is configurable when launching the platform.</p>"},{"location":"platforms/linux-workstation/#launch-configuration","title":"Launch configuration","text":"<p>Warning</p> <p>Platforms and their names are visible to all members of the cloud project!</p> Option Explanation Platform name A name to identify the Linux Workstation platform. Workstation size The size of the cloud instance to run the workstation. The options in this menu are set by the cloud operator, and the number of CPUs and quantity of RAM are displayed for each size. Volume size The size (in Gigabytes) of the cloud volume (virtual disk) available at <code>/data</code>. External IP for SSH access (optional) Use the plus button to assign an external IP address to your cloud project if the list is empty, and then select an IP to assign to your workstation."},{"location":"platforms/linux-workstation/#advanced","title":"Advanced","text":""},{"location":"platforms/linux-workstation/#platform-monitoring","title":"Platform monitoring","text":"<p>A Grafana dashboard for system monitoring is included in the platform, and is accessible from the platforms page. General current and historical system information is visible.</p>"},{"location":"platforms/linux-workstation/#eessi","title":"EESSI","text":"<p>The EESSI software suite is included in the Workstation appliance. EESSI includes a diverse collection of toolkits and modules for research computing purposes.</p> <p>A collection of EESSI demo environments are available in the EESSI demo Github repository, which can be cloned using the command below:</p> <p><code>git clone https://github.com/EESSI/eessi-demo.git</code></p> <p>The TensorFlow demo in this repository is a illustrative example of how to make use of EESSI. To run this demo, it is first neccessary to source the EESSI bash environment using the following command:</p> <p><code>source /cvmfs/pilot.eessi-hpc.org/versions/2021.12/init/bash</code></p> <p>This script will initialize the Lua modules for the Software layer in the EESSI stack and source the neccessary environment variables. To get started with EESSI, the <code>module avail</code> command will list all modules avaliable to you in the environment. A successful initialization should result in an output resembling the one below.</p> <p></p><pre><code>[EESSI pilot 2021.12] $ module avail\n\n------ /cvmfs/pilot.eessi-hpc.org/versions/2021.12/software/linux/x86_64/intel/haswell/modules/all ------\n   ant/1.10.8-Java-11\n   Arrow/0.17.1-foss-2020a-Python-3.8.2\n   Bazel/3.6.0-GCCcore-9.3.0\n   ...\n</code></pre> With a functioning environment, it is possible to begin experimenting with the software included in EESSI. Enter the directory of the EESSI-demo repository we cloned earlier, then enter the TensorFlow directory. <p></p> <p>This TensorFlow project contains a demonstration 4-layer neural network model which runs against the MNIST digits dataset. It is possible to tinker with the <code>TensorFlow-2.x_mnist-test.py</code> script to setup a different model architecture or leave it in the default configuration. To initialise the model, execute the <code>run.sh</code> script inside the directory, which should produce an evaluation output like below.</p> <p></p><pre><code>[EESSI pilot 2021.12] $ ./run.sh \n2023-10-09 13:15:04.546828: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\nEpoch 1/5\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.2951 - accuracy: 0.9138\nEpoch 2/5\n1875/1875 [==============================] - 14s 7ms/step - loss: 0.1445 - accuracy: 0.9568\nEpoch 3/5\n1875/1875 [==============================] - 6s 3ms/step - loss: 0.1070 - accuracy: 0.9669\nEpoch 4/5\n1875/1875 [==============================] - 3s 2ms/step - loss: 0.0886 - accuracy: 0.9732\nEpoch 5/5\n1875/1875 [==============================] - 3s 2ms/step - loss: 0.0766 - accuracy: 0.9763\n313/313 - 0s - loss: 0.0745 - accuracy: 0.9774\n\nreal    0m41.002s\nuser    2m24.151s\nsys     1m54.599s\n</code></pre> Guides on how to utilise EESSI further can be found here.<p></p>"},{"location":"platforms/linux-workstation/#podman","title":"Podman","text":"<p>Podman is a container framework provided in the Workstation appliance for the purpose of building and running OCI containers. It is strongly recommended to install any software for the Workstation appliance via Podman, as software installed via the package manager or otherwise located outside of <code>/home</code> will be removed during image upgrades. In-place upgrades are not supported in the Workstation appliance to avoid dependancy issues between migrations, instead the <code>/home</code> directory is kept as a seperate partition and re-mounted after the Workstation has been re-imaged.</p> <p>The Podman CLI is syntatically similar to the Docker CLI. For example, a Jupyter notebook with an accessible web interface can be deployed using Podman via the following commands.</p> <p>To begin, clone the Jupyter notebook docker repository and navigate to the notebook directory as below:</p> <p><code>git clone https://github.com/jupyter/docker-stacks.git &amp;&amp; cd docker-stacks/images/base-notebook</code></p> <p>Build the Jupyter notebook image, and select the docker.io remote when promoted by Podman:</p> <p><code>podman build . --tag jupyter-notebook</code></p> <p>Query the list of locally available images to ensure the Jupyter notebook container image was built correctly:</p> <p><code>podman image ls | grep jupyter-notebook</code></p> <p>Start the Jupyter notebook container:</p> <p><code>podman run --name notebook -p 8888:8888 jupyter-notebook</code></p> <p>Once the container has started, a link providing access to the web interface will be printed to the console. Navigating to this link in a web browser should present a Jupyter notebook web interface resembling the one shown below:</p> <p></p> <p>As this is a base notebook it won't contain many applications to explore, more notebooks to build can be found in the docker-stacks repository and may have a similar setup procedure to the base notebook.</p> <p>See the Podman docs for further information on using Podman.</p>"},{"location":"platforms/linux-workstation/#apptainer","title":"Apptainer","text":"<p>Apptainer is another container framework commonly used in HPC applications which is also included in the Workstation appliance. It can be used in a similar way to Podman as both frameworks support OCI containers. For example, OCI images from Docker Hub can be pulled with:</p> <p><code>apptainer run docker://jupyter/base-notebook</code></p> <p>Similiar to the previous Podman example, the Apptainer console log should contain an access link to the notebook interface.</p>"},{"location":"platforms/slurm/","title":"Slurm","text":""},{"location":"platforms/slurm/#introduction","title":"Introduction","text":"<p>The Slurm platform provides a multi-node HPC environment based on the Slurm workload manager and Open OnDemand. The platform is accessible with a web-browser using the Open OnDemand web-interface, or via SSH. </p>"},{"location":"platforms/slurm/#launch-configuration","title":"Launch configuration","text":"<p>Warning</p> <p>Platforms and their names are visible to all members of the cloud project!</p> Option Explanation Platform name A name to identify the Slurm platform. External IP Use the plus button to assign an external IP address to your cloud project if the list is empty, and then select an IP to assign to the login node of your Slurm platform. Compute node count The amount of Slurm compute (worker) nodes to configure for your Slurm platform. Compute node size The size of the Slurm compute (worker) nodes. The options in this menu are set by the cloud operator, and the number of CPUs and quantity of RAM are displayed for each size. Run post-configuration validation? Run a small suite of tests to check that the Slurm platform is functioning as expected."},{"location":"platforms/slurm/#advanced","title":"Advanced","text":""},{"location":"platforms/slurm/#platform-monitoring","title":"Platform monitoring","text":"<p>A Grafana dashboard for system monitoring is included in the platform, and is accessible from the platforms page. General current and historical system information is visible.</p> <p>Additionally, Open OnDemand presents monitoring dashboards for each Slurm job.</p>"},{"location":"platforms/slurm/#root-access","title":"Root access","text":"<p>The <code>azimuth</code> user has passwordless sudo. Only this user can ssh between nodes so to get sudo access to a non-login node ssh as <code>azimuth</code> from the login node first, then use sudo.</p> <p>Note that node names can be retrieved from the <code>/etc/hosts</code> file on the login node, e.g.:</p> <pre><code>cat /etc/hosts\n</code></pre>"},{"location":"platforms/slurm/#additional-software","title":"Additional software","text":"<p>Software installed directly via <code>sudo</code> will be lost when the platform is upgraded, as upgrades are performed by reimaging all nodes with a new image.</p> <p>Where possible, it is preferable to package additional software for use via apptainer which is installed on all Slurm platforms. This supports both SIF and Docker/OCI container formats.</p> <p>Some software is also available via the EESSI pilot repository - follow instructions from here.</p> <p>If these methods are not appropriate and the software has wide applicability, consider making a PR to the Ansible Slurm Appliance, which contains code for building images and configuring Slurm that is used by Azimuth.</p>"},{"location":"platforms/kubernetes-applications/argocd/","title":"ArgoCD","text":"<p>Warning</p> <p>Deploying ArgoCD requires an existing Kubernetes cluster.</p>"},{"location":"platforms/kubernetes-applications/argocd/#introduction","title":"Introduction","text":"<p>ArgoCD provides a declarative GitOps Continuous Delivery (CD) platform for Kubernetes clusters.</p>"},{"location":"platforms/kubernetes-applications/argocd/#launch-configuration","title":"Launch configuration","text":"<p>To get started, in the Platforms tab, press the  New Platform button, and select ArgoCD.</p> <p>You will then be presented with launch configuration options to fill in:</p> Option Explanation Platform name A name to identify the ArgoCD platform Kubernetes cluster The Kubernetes platform on which to deploy ArgoCD. If one hasn't already been created, check out the Kubernetes Overview. App version The version of the ArgoCD Azimuth Application to use."},{"location":"platforms/kubernetes-applications/argocd/#accessing-argocd","title":"Accessing ArgoCD","text":"<p>Once deployed, ArgoCD can be accessed in Details, using the link under Services.</p> <p>As the platform is already access controlled using the Identity Provider, upon accessing ArgoCD, you will have full access without needing to login via ArgoCD's own login system.</p>"},{"location":"platforms/kubernetes-applications/binderhub/","title":"BinderHub","text":"<p>Warning</p> <p>Deploying BinderHub requires an existing Kubernetes cluster.</p>"},{"location":"platforms/kubernetes-applications/binderhub/#introduction","title":"Introduction","text":"<p>BinderHub allows a set of Jupyter notebooks to be deployed onto a JupyterHub instance from a Git repository, under a reproducible environment. When a user accesses a BinderHub instance, a Docker image is built from the Git repository's environment configuration (e.g. <code>requirements.txt</code>, <code>envionment.yaml</code>) and a temporary Jupyter notebook server is launched. This is useful for educators and researchers looking to make their code and corresponding data available to users through their web browser; to launch, edit and run without having to install anything locally. A public example can be found at https://mybinder.org/.</p> <p></p> <p>The BinderHub Kubernetes application is installed using the Platforms dashboard of Azimuth, onto an existing Kubernetes Cluster deployment. It is automatically exposed by Zenith, and access can be granted via the Keycloak identity provider.</p>"},{"location":"platforms/kubernetes-applications/binderhub/#launch-configuration","title":"Launch configuration","text":"<p>To get started, in the Platforms tab, press the  New Platform button, and select BinderHub.</p> <p>You will then be presented with launch configuration options to fill in:</p> Option Explanation Platform name A name to identify the BinderHub platform Kubernetes cluster The Kubernetes platform on which to deploy BinderHub. If one hasn't already been created, check out the Kubernetes Overview. App version The version of the BinderHub Azimuth Application to use. Servers per user The maximum number of JupyterHub servers each user can run simultaneously. Set to 0 for unlimited. Notebook CPUs The number of CPUs to allocate to each user notebook. Notebook RAM The amount of RAM to allocate to each user notebook. Notebook storage The amount of disk storage to allocate to each user notebook. Container registry storage The storage allocated to BinderHub's docker image registry."},{"location":"platforms/kubernetes-applications/binderhub/#using-binderhub","title":"Using BinderHub","text":""},{"location":"platforms/kubernetes-applications/binderhub/#accessing-binderhub","title":"Accessing BinderHub","text":"<p>After creating the BinderHub platform, BinderHub and its corresponding JupyterHub will automatically be exposed by Azimuth's Zenith proxy.</p> <p>They can be accessed via the link under Services. </p>"},{"location":"platforms/kubernetes-applications/binderhub/#managing-users","title":"Managing Users","text":"<p>User management can be performed via the Identity Provider, Keycloak.</p> <p>Access to BinderHub and its JupyterHub can be granted by the platform or service specific Keycloak groups.</p>"},{"location":"platforms/kubernetes-applications/daskhub/","title":"DaskHub","text":"<p>Warning</p> <p>Deploying DaskHub requires an existing Kubernetes cluster.</p>"},{"location":"platforms/kubernetes-applications/daskhub/#introduction","title":"Introduction","text":"<p>DaskHub provides a multi-user, Dask-Gateway enabled JupyterHub. Dask is a library for scaling Python code across a cluster.</p>"},{"location":"platforms/kubernetes-applications/daskhub/#launch-configuration","title":"Launch configuration","text":"<p>To get started, in the Platforms tab, press the  New Platform button, and select DaskHub.</p> <p>You will then be presented with launch configuration options to fill in:</p> Option Explanation Platform name A name to identify the DaskHub platform Kubernetes cluster The Kubernetes platform on which to deploy DaskHub. If one hasn't already been created, check out the Kubernetes Overview. App version The version of the DaskHub Azimuth Application to use. Notebook CPUs The number of CPUs to allocate to each user notebook. Notebook RAM The amount of RAM to allocate to each user notebook. Notebook storage The amount of disk storage to allocate to each user notebook."},{"location":"platforms/kubernetes-applications/daskhub/#using-daskhub","title":"Using DaskHub","text":""},{"location":"platforms/kubernetes-applications/daskhub/#accessing-daskhub","title":"Accessing DaskHub","text":"<p>After creating the DaskHub platform, DaskHub's corresponding JupyterHub will automatically be exposed by Azimuth's Zenith proxy.</p> <p>It can be accessed via the link under Services. </p>"},{"location":"platforms/kubernetes-applications/daskhub/#using-dask-gateway","title":"Using Dask Gateway","text":"<p>Dask Gateway is configured to integrate with the JupyterHub environment, so creating a Dask cluster requires very little configuration.</p> <p>For example, the following creates a Dask cluster that scales between 0 and 10 workers on the underlying Kubernetes cluster depending on the workload:</p> <pre><code>from dask_gateway import Gateway\n\ngateway = Gateway()\ncluster = gateway.new_cluster()\ncluster.adapt(minimum=0, maximum=10)\ncluster\n</code></pre> <p>If your Kubernetes cluster has autoscaling of nodes configured, this may cause the cluster itself to grow in size to accomodate your Dask cluster. Once you have finished with the Dask cluster, the Kubernetes nodes will scale back down again when possible.</p>"},{"location":"platforms/kubernetes-applications/huggingface-llm/","title":"Huggingface llm","text":"<p>Warning</p> <p>Deploying Huggingface-LLM requires an existing Kubernetes cluster, with a GPU-enabled node group.</p> <p>Bug</p> <p>If HuggingFace is deployed onto a cluster without a GPU-enabled node group, or there is some vLLM error during deployment, it may get stuck in the \"Installing\" state and refuse to uninstall. If this happens, delete the <code>huggingface-api</code> object and/or the entire associated namespace via <code>kubectl</code> or the Kubernetes Dashboard. The uninstall process should then successfully complete.</p>"},{"location":"platforms/kubernetes-applications/huggingface-llm/#introduction","title":"Introduction","text":"<p>A generative AI chatbot service backed by a HuggingFace model, exposed via a convenient web interface.</p>"},{"location":"platforms/kubernetes-applications/huggingface-llm/#launch-configuration","title":"Launch configuration","text":"<p>To get started, in the Platforms tab, press the  New Platform button, and select HuggingFace LLM.</p> <p>Warning</p> <p>Please be conscious of UKRI and the STFC's security policies when selecting a model. For example, DeepSeek cannot be used on the STFC Cloud Platform.</p> <p>You will then be presented with launch configuration options to fill in:</p> Option Explanation Platform name A name to identify the HuggingFace LLM platform Kubernetes cluster The Kubernetes platform on which to deploy HuggingFace LLM. If one hasn't already been created, check out the Kubernetes Overview. App version The version of the HuggingFace LLM Azimuth Application to use. Model The model to deploy from HuggingFace. vLLM is used for model serving, so any of their supported models should work. Access Token HuggingFace https://huggingface.co/docs/hub/security-tokens which is required for some gated models Instruction The initial system prompt, hidden from the user, which is used when generating responses Page Title The title displayed at the top of the chat interface Backend vLLM Version The version of vLLM to use from this list LLM Sampling Parameters (Temperature, Frequency etc) See the vLLM docs Max Tokens Maximum number of tokens to generate per response. Use this to moderate compute cost. Model Context Length Override for the model's maximum context length <p>It may take a long time for HuggingFace to install. Progress can be checked via the <code>huggingface-api</code> pod's logs. These can be accessed in <code>kubectl</code> or the Kubernetes Dashboard.</p>"},{"location":"platforms/kubernetes-applications/jupyterhub/","title":"JupyterHub","text":"<p>Warning</p> <p>Deploying JupyterHub requires an existing Kubernetes cluster.</p>"},{"location":"platforms/kubernetes-applications/jupyterhub/#introduction","title":"Introduction","text":"<p>JupyterHub allows users to interact with a computing environment through a webpage. As most devices have access to a web browser, JupyterHub makes it is easy to provide and standardize the computing environment for a group of people (e.g., for a class of students or an analytics team). JupyterHub is a web-interface through which many \"singleuser\" Jupyter notebooks (one per user) may be launched.</p> <p>Upon accessing JupyterHub users are presented with a number of \"Profiles\": pre-installed Python, Julia and R environments aimed towards different use cases.</p> <p></p> <p>The JupyterHub Kubernetes application is installed using the Platforms dashboard of Azimuth, onto an existing Kubernetes Cluster deployment.</p>"},{"location":"platforms/kubernetes-applications/jupyterhub/#launch-configuration","title":"Launch configuration","text":"<p>To get started, in the Platforms tab, press the  New Platform button, and select JupyterHub.</p> <p>You will then be presented with launch configuration options to fill in:</p> Option Explanation Platform name A name to identify the JupyterHub platform Kubernetes cluster The Kubernetes platform on which to deploy JupyterHub. If one hasn't already been created, check out the Kubernetes Overview. App version The version of the JupyterHub Azimuth application to use. Notebook CPUs The number of CPUs to allocate to each user notebook. Notebook RAM The amount of RAM to allocate to each user notebook. Notebook storage The amount of disk storage to allocate to each user notebook."},{"location":"platforms/kubernetes-applications/jupyterhub/#using-jupyterhub","title":"Using JupyterHub","text":""},{"location":"platforms/kubernetes-applications/jupyterhub/#accessing-jupyterhub","title":"Accessing JupyterHub","text":"<p>After creating the JupyterHub platform, it will automatically be exposed by Azimuth's Zenith proxy.</p> <p>JupyterHub can be accessed via the link under Services. </p> <p>Users should be prompted to start a server, and be given three default options. </p> <p>Note</p> <p>Further development will allow for a custom image and/or a whole custom repo-to-docker codebase to be used</p>"},{"location":"platforms/kubernetes-applications/jupyterhub/#profiles","title":"Profiles","text":"Profile Description Python environment (minimal) A minimally functional Jupyter server; with basic commandline tools, a TeX environment and Python (along with the Conda and Mamba package managers). Data Science Notebook Contains popular scientific Python packages, the R interpreter and a basic set of data science packages, rpy2, and the Julia compiler along with notebook support and HDF5. GPU-enabled Machine Learning environment Contains GPU support and drivers; along with TensorFlow, Keras, PyTorch, and everything in the above Data Science Notebook. <p>Warning</p> <p>The <code>GPU-enabled Machine Learning environment</code> profile will only launch on clusters with a node group that has machines with a GPU-enabled flavour. It may take 10-15 minutes to launch this profile for the first time on a newly deployed GPU node. This is because Nvidia CUDA drivers are installed first automatically in the background, then the large ~8GB profile docker image needs to be downloaded and loaded.While waiting, you may see a message saying \"No Capacity\" or \"No Nodes match the affinity/selector\". This is okay, so long as there is a GPU node group available.</p>"},{"location":"platforms/kubernetes-applications/jupyterhub/#server-hub","title":"Server Hub","text":"<p>To switch profile or restart the code server from JupyterLab, head to File \u2192 Hub Control Panel.</p> <p>From there, the server may be stopped and relaunched.</p>"},{"location":"platforms/kubernetes-applications/jupyterhub/#managing-users","title":"Managing Users","text":"<p>User management can be performed via the Identity Provider, Keycloak.</p> <p>Access to JupyterHub can be granted by the platform or service specific Keycloak groups.</p>"},{"location":"platforms/kubernetes-applications/kubeflow/","title":"KubeFlow","text":"<p>Warning</p> <p>Deploying KubeFlow requires an existing Kubernetes cluster containing a node group with at least 16GB+ of RAM and 12+ CPUs, and ideally with a GPU.</p> <p>Deployment takes a long time as many large images need to be pulled. Progress can be followed via the Kubernetes Dashboard in the Workloads tab of the Kubeflow namespace, or via <code>kubectl</code>.</p>"},{"location":"platforms/kubernetes-applications/kubeflow/#introduction","title":"Introduction","text":"<p>KubeFlow is a machine learning toolkit for Kubernetes clusters, using Jupyter Notebooks and TensorFlow.</p> <p>For an introduction to using KubeFlow, see the official documentation.</p> <p>Warning</p> <p>The KubeFlow app deployment is currently at a proof-of-concept stage and does not yet provide full integration with Azimuth's standard authentication and access management features. Full integration with the Azimuth identity provider is planned for a future release.</p>"},{"location":"platforms/kubernetes-applications/kubeflow/#launch-configuration","title":"Launch configuration","text":"<p>To get started, in the Platforms tab, press the  New Platform button, and select KubeFlow.</p> <p>KubeFlow requires a worker node cluster with 16GB+ of RAM and 12+ CPUs. Ideally, it should be a GPU flavor.</p> <p>You will then be presented with launch configuration options to fill in:</p> Option Explanation Platform name A name to identify the KubeFlow platform Kubernetes cluster The Kubernetes platform on which to deploy KubeFlow. If one hasn't already been created, check out the Kubernetes Overview. App version The version of the KubeFlow Azimuth Application to use."},{"location":"platforms/kubernetes-applications/kubeflow/#accessing-kubeflow","title":"Accessing KubeFlow","text":"<p>The default login credentials for KubeFlow are:</p> <ul> <li>username: user@example.com </li> <li>password: 12341234</li> </ul>"},{"location":"platforms/kubernetes-cluster-addons/kubernetes_dashboard/","title":"Kubernetes Dashboard","text":""},{"location":"platforms/kubernetes-cluster-addons/kubernetes_dashboard/#introduction","title":"Introduction","text":"<p>The Kubernetes Dashboard Web UI is available for deployed Kubernetes platforms on Azimuth. This provides a user-friendly way to administer a cluster.</p> <p>For usage information, see the Kubernetes Docs.</p> <p></p>"},{"location":"platforms/kubernetes-cluster-addons/kubernetes_dashboard/#accessing-kubernetes-dashboards","title":"Accessing Kubernetes Dashboards","text":"<p>Kubernetes Dashboard can be accessed via the link in the cluster details </p> <p>Kubernetes Dashboard can be enabled/disabled on an existing cluster using the green  <code>Update</code> button in details, under the Cluster Addons section.</p> <p>Access can be granted through the Identity Provider.</p> <p>Info</p> <p>We recommend access to administrating clusters, through the Kubernetes Dashboard or through the CLI (Kubectl), be limited to users who already have admin access via your OpenStack project. This is because users with access can create, delete or modify resources in your cluster(s).If granting access to other users is necessary, we recommend it is only to the Kubernetes Dashboard via the Identity Provider as this can be revoked later. Permission to generate a kubeconfig file for CLI usage cannot be granted through azimuth to non-admin users, the kubeconfig file can only be shared manually which we do not recommend.</p>"},{"location":"platforms/kubernetes-cluster-addons/monitoring/","title":"Monitoring","text":""},{"location":"platforms/kubernetes-cluster-addons/monitoring/#introduction","title":"Introduction","text":"<p>Monitoring of user-deployed Kubernetes clusters is provided through a Prometheus, Grafana, AlertManager, Loki, Fluent-Operator stack.</p> <p>This can all be accessed through Grafana, which can be found via the \"Monitoring\" link in the Details page of your Kubernetes Cluster.</p> <p></p> <p>The set of Monitoring cluster addons can be enabled/disabled on an existing cluster using the green  <code>Update</code> button in details, under the Cluster Addons section.</p> <p>Info</p> <p>Even if the Monitoring cluster addon is disabled, Fluent-Operator will continue collecting logs to our central logging server as per our security compliance policy.</p> <p>Access to Grafana can be granted through the Identity Provider</p>"},{"location":"platforms/kubernetes-cluster-addons/monitoring/#accessing-dashboards","title":"Accessing Dashboards","text":"<p>Grafana dashboards are an easy and accessible way to query and visualize data.</p> <p>Azimuth provides a number of useful dashboards by default to access your logging, metrics and alerting. These can be found in Grafana under the \"Dashboards\" sidebar entry. New dashboards can also be created, see the Grafana Docs.</p> <p></p>"},{"location":"platforms/kubernetes-cluster-addons/monitoring/#accessing-metrics","title":"Accessing Metrics","text":"<p>The easiest way to browse Azimuth metrics is via the Dashboards.</p> <p>However they can also be accessed under Drilldown \u2192 Metrics at a glance, or queried under Explore \u2192 Prometheus.</p> <p></p>"},{"location":"platforms/kubernetes-cluster-addons/monitoring/#accessing-logs","title":"Accessing Logs","text":"<p>The best place to examine logs is via Explore -&gt; Loki </p>"},{"location":"platforms/kubernetes-cluster-addons/monitoring/#accessing-alerting","title":"Accessing Alerting","text":"<p>AlertManager is managed through Grafana, via the Alerting tab. Alert Rules, Silences, and Notifications can be configured. Alerts can also be viewed.</p>"},{"location":"quotas/quotas/","title":"Quotas","text":"<p>Warning</p> <p>The list of quotas Azimuth displays here are incomplete. For a complete overview of quota usage, please instead use the OpenStack Web UI (Horizon).</p> <p>Each cloud project is assigned a cloud resources quota. This page displays the consumption of your quota by your existing cloud resources.</p> <p></p>"}]}